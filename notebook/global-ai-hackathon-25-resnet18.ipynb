{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":94147,"databundleVersionId":11390004,"sourceType":"competition"}],"dockerImageVersionId":30918,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# 1. Data Loading and Visualization","metadata":{}},{"cell_type":"code","source":"import h5py\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport torch\nimport torch.nn as nn\nfrom torch.utils.data import Dataset, DataLoader\nfrom torchvision import transforms, models\nfrom tqdm import tqdm\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-14T07:45:33.116366Z","iopub.execute_input":"2025-05-14T07:45:33.116734Z","iopub.status.idle":"2025-05-14T07:45:33.126704Z","shell.execute_reply.started":"2025-05-14T07:45:33.116702Z","shell.execute_reply":"2025-05-14T07:45:33.125391Z"}},"outputs":[{"name":"stdout","text":"/kaggle/input/el-hackathon-2025/elucidata_ai_challenge_data.h5\n","output_type":"stream"}],"execution_count":21},{"cell_type":"code","source":"h5_path = '/kaggle/input/el-hackathon-2025/elucidata_ai_challenge_data.h5'\nh5_file = h5py.File(h5_path, 'r')\n\n# Load image and spot handles\ntrain_images = h5_file['images']['Train']\ntrain_spots = h5_file['spots']['Train']\n\ntest_image = h5_file['images']['Test']['S_7'][()]\ntest_spots_df = pd.DataFrame(h5_file['spots']['Test']['S_7'][:])\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-14T07:45:33.128315Z","iopub.execute_input":"2025-05-14T07:45:33.128707Z","iopub.status.idle":"2025-05-14T07:45:33.190201Z","shell.execute_reply.started":"2025-05-14T07:45:33.128667Z","shell.execute_reply":"2025-05-14T07:45:33.188823Z"}},"outputs":[],"execution_count":22},{"cell_type":"markdown","source":"# 2. Extract Patches and Labels","metadata":{}},{"cell_type":"code","source":"def extract_patch(image, x, y, patch_size=224):\n    half = patch_size // 2\n    x_min = max(x - half, 0)\n    x_max = min(x + half, image.shape[1])\n    y_min = max(y - half, 0)\n    y_max = min(y + half, image.shape[0])\n\n    patch = image[y_min:y_max, x_min:x_max]\n\n    # Pad if patch is smaller than expected\n    if patch.shape[0] < patch_size or patch.shape[1] < patch_size:\n        pad_y = patch_size - patch.shape[0]\n        pad_x = patch_size - patch.shape[1]\n        patch = np.pad(patch, ((0, pad_y), (0, pad_x), (0, 0)), mode='constant')\n    \n    return patch","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-14T07:45:33.192261Z","iopub.execute_input":"2025-05-14T07:45:33.192685Z","iopub.status.idle":"2025-05-14T07:45:33.200751Z","shell.execute_reply.started":"2025-05-14T07:45:33.192628Z","shell.execute_reply":"2025-05-14T07:45:33.198974Z"}},"outputs":[],"execution_count":23},{"cell_type":"code","source":"train_patches = []\ntrain_labels = []\n\nfor i in range(1, 7): \n    slide_id = f'S_{i}'\n    image = train_images[slide_id][()]\n    spots_df = pd.DataFrame(train_spots[slide_id][:])\n\n    for _, row in tqdm(spots_df.iterrows(), total=len(spots_df)):\n        x, y = int(row['x']), int(row['y'])\n        label = [row[f'C{j}'] for j in range(1, 36)]\n        patch = extract_patch(image, x, y)\n        train_patches.append(patch)\n        train_labels.append(label)\n\ntrain_patches = np.array(train_patches)\ntrain_labels = np.array(train_labels)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-14T07:45:33.202511Z","iopub.execute_input":"2025-05-14T07:45:33.202892Z","iopub.status.idle":"2025-05-14T07:45:36.956191Z","shell.execute_reply.started":"2025-05-14T07:45:33.202848Z","shell.execute_reply":"2025-05-14T07:45:36.954923Z"}},"outputs":[{"name":"stderr","text":"100%|██████████| 2197/2197 [00:00<00:00, 6241.38it/s]\n100%|██████████| 2270/2270 [00:00<00:00, 6102.85it/s]\n100%|██████████| 690/690 [00:00<00:00, 5892.09it/s]\n100%|██████████| 1187/1187 [00:00<00:00, 5784.97it/s]\n100%|██████████| 1677/1677 [00:00<00:00, 6053.08it/s]\n100%|██████████| 328/328 [00:00<00:00, 5915.24it/s]\n","output_type":"stream"}],"execution_count":24},{"cell_type":"markdown","source":"# 3. Define Dataset and DataLoader","metadata":{}},{"cell_type":"code","source":"class SpotDataset(Dataset):\n    def __init__(self, patches, labels, transform=None):\n        self.patches = patches\n        self.labels = labels\n        self.transform = transform\n        \n    def __len__(self):\n        return len(self.patches)\n    \n    def __getitem__(self, idx):\n        img = self.patches[idx].astype(np.uint8)\n        img = transforms.ToPILImage()(img)\n        if self.transform:\n            img = self.transform(img)\n        label = torch.tensor(self.labels[idx], dtype=torch.float32)\n        return img, label","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-14T07:45:36.957610Z","iopub.execute_input":"2025-05-14T07:45:36.958050Z","iopub.status.idle":"2025-05-14T07:45:36.965764Z","shell.execute_reply.started":"2025-05-14T07:45:36.957986Z","shell.execute_reply":"2025-05-14T07:45:36.964311Z"}},"outputs":[],"execution_count":25},{"cell_type":"code","source":"train_transform = transforms.Compose([\n    transforms.Resize((224, 224)),\n    transforms.ToTensor()\n])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-14T07:45:36.966856Z","iopub.execute_input":"2025-05-14T07:45:36.967240Z","iopub.status.idle":"2025-05-14T07:45:36.986831Z","shell.execute_reply.started":"2025-05-14T07:45:36.967195Z","shell.execute_reply":"2025-05-14T07:45:36.985649Z"}},"outputs":[],"execution_count":26},{"cell_type":"markdown","source":"# 4. Initialize Model","metadata":{}},{"cell_type":"code","source":"def get_model():\n    model = models.resnet18(pretrained=True)\n    model.fc = nn.Linear(model.fc.in_features, 35)\n    return model","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-14T07:45:36.987982Z","iopub.execute_input":"2025-05-14T07:45:36.988579Z","iopub.status.idle":"2025-05-14T07:45:37.014254Z","shell.execute_reply.started":"2025-05-14T07:45:36.988538Z","shell.execute_reply":"2025-05-14T07:45:37.012979Z"}},"outputs":[],"execution_count":27},{"cell_type":"markdown","source":"# Train Loop","metadata":{}},{"cell_type":"code","source":"device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\ndataset = SpotDataset(train_patches, train_labels, transform=train_transform)\ndataloader = DataLoader(dataset, batch_size=32, shuffle=True)\n\nmodel = get_model().to(device)\noptimizer = torch.optim.AdamW(model.parameters(), lr=1e-4)\nloss_fn = nn.MSELoss()\n\nfor epoch in range(5): \n    model.train()\n    total_loss = 0\n    for imgs, targets in tqdm(dataloader):\n        imgs, targets = imgs.to(device), targets.to(device)\n        optimizer.zero_grad()\n        preds = model(imgs)\n        loss = loss_fn(preds, targets)\n        loss.backward()\n        optimizer.step()\n        total_loss += loss.item()\n    print(f\"Epoch {epoch+1} Loss: {total_loss / len(dataloader)}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-14T07:45:37.016758Z","iopub.execute_input":"2025-05-14T07:45:37.017157Z","iopub.status.idle":"2025-05-14T09:11:06.731199Z","shell.execute_reply.started":"2025-05-14T07:45:37.017122Z","shell.execute_reply":"2025-05-14T09:11:06.729633Z"}},"outputs":[{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n  warnings.warn(msg)\nDownloading: \"https://download.pytorch.org/models/resnet18-f37072fd.pth\" to /root/.cache/torch/hub/checkpoints/resnet18-f37072fd.pth\n100%|██████████| 44.7M/44.7M [00:00<00:00, 177MB/s]\n100%|██████████| 261/261 [17:15<00:00,  3.97s/it]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 1 Loss: 0.8410746984440705\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 261/261 [17:00<00:00,  3.91s/it]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 2 Loss: 0.5583732446148935\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 261/261 [16:51<00:00,  3.88s/it]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 3 Loss: 0.4491480861815456\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 261/261 [17:16<00:00,  3.97s/it]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 4 Loss: 0.35784392741105564\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 261/261 [17:03<00:00,  3.92s/it]","output_type":"stream"},{"name":"stdout","text":"Epoch 5 Loss: 0.29651627278533477\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"}],"execution_count":28},{"cell_type":"code","source":"spot_df = pd.DataFrame(h5_file['spots']['Test']['S_7'][:])\nspot_ids = [f'spot_{i}' for i in range(len(spot_df))]\n\nmodel.eval()\npredictions = []\n\nwith torch.no_grad():\n    for _, row in tqdm(spot_df.iterrows(), total=len(spot_df)):\n        x, y = int(row['x']), int(row['y'])\n        patch = extract_patch(test_image, x, y)\n        img = transforms.ToPILImage()(patch.astype(np.uint8))\n        img = train_transform(img).unsqueeze(0).to(device)\n        pred = model(img).cpu().numpy().flatten()\n        predictions.append(pred)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-14T09:14:26.219591Z","iopub.execute_input":"2025-05-14T09:14:26.219961Z","iopub.status.idle":"2025-05-14T09:16:15.151570Z","shell.execute_reply.started":"2025-05-14T09:14:26.219927Z","shell.execute_reply":"2025-05-14T09:16:15.150419Z"}},"outputs":[{"name":"stderr","text":"100%|██████████| 2088/2088 [01:48<00:00, 19.17it/s]\n","output_type":"stream"}],"execution_count":32},{"cell_type":"markdown","source":"# Create Submission File","metadata":{}},{"cell_type":"code","source":"submission_df = pd.DataFrame(predictions, columns=[f'C{i}' for i in range(1, 36)])\nsubmission_df.insert(0, 'ID', spot_ids)\nsubmission_df.to_csv('submission.csv', index=False)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-14T09:16:31.912250Z","iopub.execute_input":"2025-05-14T09:16:31.912692Z","iopub.status.idle":"2025-05-14T09:16:32.031805Z","shell.execute_reply.started":"2025-05-14T09:16:31.912658Z","shell.execute_reply":"2025-05-14T09:16:32.030713Z"}},"outputs":[],"execution_count":35}]}